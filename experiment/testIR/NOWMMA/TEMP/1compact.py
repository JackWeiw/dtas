from tvm.script import ir as I
from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), "int8"), lv4: T.Buffer((T.int64(2560),), "int8"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n = T.int64()
        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), "int8")
        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), "int8")
        # with T.block("root"):
        for blockIdx_y in T.thread_binding(T.int64(40), thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding((n + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for vthread_y in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for vthread_x in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for threadIdx_y in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                            for threadIdx_x in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                with T.block(""):
                                    T.reads(lv9[T.int64(0), blockIdx_x * T.int64(32):blockIdx_x * T.int64(32) + T.int64(64), T.int64(0):T.int64(10240)], lv3[blockIdx_y * T.int64(64):blockIdx_y * T.int64(64) + T.int64(64), T.int64(0):T.int64(10240)], lv4[blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4):blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + T.int64(4)])
                                    T.writes(var_T_add_intermediate[T.int64(0), blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4):blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + T.int64(4), blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4):blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + T.int64(4)])
                                    var_NT_matmul_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(4)), "int8", scope="local")
                                    for ax2_3_init, ax1_3_init in T.grid(T.int64(4), T.int64(4)):
                                        with T.block("NT_matmul_init"):
                                            T.reads()
                                            T.writes(var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3_init - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3_init - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))])
                                            var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3_init - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3_init - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))] = T.int8(0)
                                    for ax3_0 in range(T.int64(640)):
                                        with T.block(""):
                                            T.reads(lv9[T.int64(0), blockIdx_x * T.int64(32):blockIdx_x * T.int64(32) + T.int64(64), ax3_0 * T.int64(16):ax3_0 * T.int64(16) + T.int64(16)], lv3[blockIdx_y * T.int64(64):blockIdx_y * T.int64(64) + T.int64(64), ax3_0 * T.int64(16):ax3_0 * T.int64(16) + T.int64(16)], var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)):blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)) + T.int64(4), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)):blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)) + T.int64(4)])
                                            T.writes(var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)):blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)) + T.int64(4), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)):blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)) + T.int64(4)])
                                            lv9_reindex_pad_shared = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(16)), "int8", strides=(T.int64(512), T.int64(16), T.int64(1)), scope="shared")
                                            lv3_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(16)), "int8", strides=(T.int64(1024), T.int64(16), T.int64(1)), scope="shared")
                                            lv9_reindex_pad_shared_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(8)), "int8", scope="local")
                                            lv3_reindex_shared_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(8)), "int8", scope="local")
                                            for ax0, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(1)):
                                                for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={"check_vector_load": 1, "remove_vector_condition": 1}):
                                                    with T.block(""):
                                                        T.where(ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y < T.int64(8))
                                                        T.reads(lv9[T.int64(0), blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)])
                                                        T.writes(lv9_reindex_pad_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1])
                                                        lv9_reindex_pad_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1] = T.if_then_else(blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16) < n, lv9[T.int64(0), blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)], T.int8(0))
                                            for ax0, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(1)):
                                                for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={"check_vector_load": 1, "remove_vector_condition": 1}):
                                                    with T.block("lv9_reindex_pad_shared"):
                                                        T.where(ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y < T.int64(8))
                                                        T.reads(lv9[T.int64(0), blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)])
                                                        T.writes(lv9_reindex_pad_shared[ax0, blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16) - blockIdx_x * T.int64(32), ax3_0 * T.int64(16) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16) - ax3_0 * T.int64(16)])
                                                        T.block_attr({"buffer_dim_align": [[0, 1, 8, 8]], "double_buffer_scope": 0})
                                                        lv9_reindex_pad_shared[ax0, blockIdx_x * T.int64(32) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16) - blockIdx_x * T.int64(32), ax3_0 * T.int64(16) + ((ax1_ax2_fused_0_0_0 * T.int64(16) + threadIdx_y) * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16) - ax3_0 * T.int64(16)] = lv9_reindex_pad_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1]
                                            for ax0, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(1)):
                                                for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={"check_vector_load": 1, "remove_vector_condition": 1}):
                                                    with T.block(""):
                                                        T.reads(lv3[blockIdx_y * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)])
                                                        T.writes(lv3_reindex_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1])
                                                        lv3_reindex_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1] = lv3[blockIdx_y * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)]
                                            for ax0, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(1)):
                                                for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={"check_vector_load": 1, "remove_vector_condition": 1}):
                                                    with T.block("lv3_reindex_shared"):
                                                        T.reads(lv3[blockIdx_y * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16), ax3_0 * T.int64(16) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16)])
                                                        T.writes(lv3_reindex_shared[ax0, blockIdx_y * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16) - blockIdx_y * T.int64(64), ax3_0 * T.int64(16) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16) - ax3_0 * T.int64(16)])
                                                        T.block_attr({"buffer_dim_align": [[0, 1, 8, 8]], "double_buffer_scope": 0})
                                                        lv3_reindex_shared[ax0, blockIdx_y * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) // T.int64(16) - blockIdx_y * T.int64(64), ax3_0 * T.int64(16) + (ax1_ax2_fused_0_0_0 * T.int64(1024) + threadIdx_y * T.int64(64) + threadIdx_x * T.int64(8) + ax1_ax2_fused_1) % T.int64(16) - ax3_0 * T.int64(16)] = lv3_reindex_shared_local[ax0, ax1_ax2_fused_0_0_0, ax1_ax2_fused_1]
                                            for ax3_1 in range(T.int64(16)):
                                                with T.block(""):
                                                    T.reads(lv9_reindex_pad_shared[T.int64(0), blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) - blockIdx_x * T.int64(32):blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) - blockIdx_x * T.int64(32) + T.int64(4), ax3_0 * T.int64(16) + ax3_1 - ax3_0 * T.int64(16)], lv3_reindex_shared[T.int64(0), blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) - blockIdx_y * T.int64(64):blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) - blockIdx_y * T.int64(64) + T.int64(4), ax3_0 * T.int64(16) + ax3_1 - ax3_0 * T.int64(16)], var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)):blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)) + T.int64(4), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)):blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)) + T.int64(4)])
                                                    T.writes(var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)):blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)) + T.int64(4), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)):blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)) + T.int64(4)])
                                                    lv9_reindex_pad_shared_local_1 = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(1)), "int8", scope="local")
                                                    lv3_reindex_shared_local_1 = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(1)), "int8", scope="local")
                                                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(1)):
                                                        with T.block("lv9_reindex_pad_shared_local"):
                                                            T.reads(lv9_reindex_pad_shared[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - blockIdx_x * T.int64(32), ax3_0 * T.int64(16) + ax3_1 + ax2 - ax3_0 * T.int64(16)])
                                                            T.writes(lv9_reindex_pad_shared_local_1[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 + ax2 - (ax3_0 * T.int64(16) + ax3_1)])
                                                            lv9_reindex_pad_shared_local_1[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 + ax2 - (ax3_0 * T.int64(16) + ax3_1)] = lv9_reindex_pad_shared[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - blockIdx_x * T.int64(32), ax3_0 * T.int64(16) + ax3_1 + ax2 - ax3_0 * T.int64(16)]
                                                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(1)):
                                                        with T.block("lv3_reindex_shared_local"):
                                                            T.reads(lv3_reindex_shared[ax0, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax1 - blockIdx_y * T.int64(64), ax3_0 * T.int64(16) + ax3_1 + ax2 - ax3_0 * T.int64(16)])
                                                            T.writes(lv3_reindex_shared_local_1[ax0, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax1 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 + ax2 - (ax3_0 * T.int64(16) + ax3_1)])
                                                            lv3_reindex_shared_local_1[ax0, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax1 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 + ax2 - (ax3_0 * T.int64(16) + ax3_1)] = lv3_reindex_shared[ax0, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax1 - blockIdx_y * T.int64(64), ax3_0 * T.int64(16) + ax3_1 + ax2 - ax3_0 * T.int64(16)]
                                                    for ax2_3, ax1_3 in T.grid(T.int64(4), T.int64(4)):
                                                        with T.block("NT_matmul_update"):
                                                            T.reads(var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))], lv9_reindex_pad_shared_local_1[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 - (ax3_0 * T.int64(16) + ax3_1)], lv3_reindex_shared_local_1[T.int64(0), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 - (ax3_0 * T.int64(16) + ax3_1)])
                                                            T.writes(var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))])
                                                            var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))] = var_NT_matmul_intermediate_reindex_pad_local[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))] + lv9_reindex_pad_shared_local_1[T.int64(0), blockIdx_x * T.int64(32) + vthread_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1_3 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 - (ax3_0 * T.int64(16) + ax3_1)] * lv3_reindex_shared_local_1[T.int64(0), blockIdx_y * T.int64(64) + vthread_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2_3 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4)), ax3_0 * T.int64(16) + ax3_1 - (ax3_0 * T.int64(16) + ax3_1)]
                                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                        with T.block("var_NT_matmul_intermediate_reindex_pad_local"):
                                            T.reads(var_NT_matmul_intermediate_reindex_pad_local[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))], lv4[blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2])
                                            T.writes(var_T_add_intermediate[T.int64(0), blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2])
                                            if blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 < n:
                                                var_T_add_intermediate[T.int64(0), blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1, blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2] = var_NT_matmul_intermediate_reindex_pad_local[ax0, blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4) + ax1 - (blockIdx_x * T.int64(32) + threadIdx_x * T.int64(4)), blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2 - (blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4))] + lv4[blockIdx_y * T.int64(64) + threadIdx_y * T.int64(4) + ax2]