[
    {
        "fused_softmax_cast_<n: Range(1, 512)>": {
            "latency(ms)": 0.0309643474161378,
            "config": [
                {
                    "len_tx": 1024,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache",
                    "max_active_blocks_per_sm": 0
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(1024) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(1024) + ax1_fused_1 < n)\n                            T.reads(A[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(1024) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(1024) + ax1_fused_1 < n)\n                            T.reads(A[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(1024) + ax1_1)\n                        T.where(ax1_0 * T.int64(1024) + ax1_1 < n)\n                        T.reads(A[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(513, 1024)>": {
            "latency(ms)": 0.03142747799136916,
            "config": [
                {
                    "len_tx": 1024,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache",
                    "max_active_blocks_per_sm": 0
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(1024) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(1024) + ax1_fused_1 < n)\n                            T.reads(A[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(1024) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(1024) + ax1_fused_1 < n)\n                            T.reads(A[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(1023)) // T.int64(1024), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(1024) + ax1_1)\n                        T.where(ax1_0 * T.int64(1024) + ax1_1 < n)\n                        T.reads(A[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(1025, 1536)>": {
            "latency(ms)": 0.01552326297709924,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 12
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(512) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(128) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(128) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(128) + ax1_1)\n                        T.where(ax1_0 * T.int64(128) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(1537, 2048)>": {
            "latency(ms)": 0.023320012579093902,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 12
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(512) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(128) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (n + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(128) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(128) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(128) + ax1_1)\n                        T.where(ax1_0 * T.int64(128) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(2049, 2560)>": {
            "latency(ms)": 0.026265973469960697,
            "config": [
                {
                    "len_tx": 160,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 9
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):\n                for ax2_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(640) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax2_0 * T.int64(160) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax2_0 * T.int64(160) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(160) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (n + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(160) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(160) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(160) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(160) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(160) + ax1_1)\n                        T.where(ax1_0 * T.int64(160) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(2561, 3072)>": {
            "latency(ms)": 0.03059306866020715,
            "config": [
                {
                    "len_tx": 192,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 8
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):\n                for ax2_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(768) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(192) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(192) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(192) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(192) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(192) + ax1_1)\n                        T.where(ax1_0 * T.int64(192) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(3073, 3584)>": {
            "latency(ms)": 0.03562889726923077,
            "config": [
                {
                    "len_tx": 192,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 7
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):\n                for ax2_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(768) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(192) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (n + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(192) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(192) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(192) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(192) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(192) + ax1_1)\n                        T.where(ax1_0 * T.int64(192) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    },
    {
        "fused_softmax_cast_<n: Range(3585, 4096)>": {
            "latency(ms)": 0.042369421875,
            "config": [
                {
                    "len_tx": 256,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn",
                    "max_active_blocks_per_sm": 6
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_A: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        A = T.match_buffer(p_A, (T.int64(1), T.int64(1000), n))\n        compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1000), n), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1000)), scope=\"shared\")\n        A_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(1000), n), scope=\"shared.dyn\")\n        assert n > T.int64(0), \"[n] should be greater than 0\"\n        for ax0_fused in T.thread_binding(T.int64(1000), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), (T.max(T.int64(1), (n + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):\n                for ax2_1 in T.thread_binding(T.int64(256), thread=\"threadIdx.x\"):\n                    for ax2_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"A_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(1000), ax0_fused + ax1)\n                            v2 = T.axis.spatial(n, ax2_0 * T.int64(1024) + ax2_1 * T.int64(4) + ax2_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax2_0 * T.int64(256) + ax2_1) * T.int64(4) + ax2_2) and T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax2_0 * T.int64(256) + ax2_1) * T.int64(4) + ax2_2) < n and (ax2_0 * T.int64(256) + ax2_1) * T.int64(4) + ax2_2 < T.max(T.int64(1), (n + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (n + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))\n                            T.reads(A[v0, v1, v2])\n                            T.writes(A_shared_dyn[v0, v1, v2])\n                            A_shared_dyn[v0, v1, v2] = A[v0, v1, v2]\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(256), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(255)) // T.int64(256), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(256) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0] = T.max(T_softmax_maxelem_shared[T.int64(0), v0], A_shared_dyn[T.int64(0), v0, v1])\n            for ax0 in range(T.int64(1)):\n                for ax1_fused_1 in T.thread_binding(T.int64(256), thread=\"threadIdx.x\"):\n                    for ax1_fused_0 in T.serial((n + T.int64(255)) // T.int64(256), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(1000), ax0_fused + ax0)\n                            v1 = T.axis.reduce(n, ax1_fused_0 * T.int64(256) + ax1_fused_1)\n                            T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < n)\n                            T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0] = T_softmax_expsum_shared[T.int64(0), v0] + T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0])\n            for ax1_1 in T.thread_binding(T.int64(256), thread=\"threadIdx.x\"):\n                for ax1_0 in T.serial((n + T.int64(255)) // T.int64(256), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(1000), ax0_fused)\n                        v1 = T.axis.spatial(n, ax1_0 * T.int64(256) + ax1_1)\n                        T.where(ax1_0 * T.int64(256) + ax1_1 < n)\n                        T.reads(A_shared_dyn[T.int64(0), v0, v1], T_softmax_maxelem_shared[T.int64(0), v0], T_softmax_expsum_shared[T.int64(0), v0])\n                        T.writes(compute_intermediate[T.int64(0), v0, v1])\n                        compute_intermediate[T.int64(0), v0, v1] = T.Cast(\"float16\", T.exp(A_shared_dyn[T.int64(0), v0, v1] - T_softmax_maxelem_shared[T.int64(0), v0]) / T_softmax_expsum_shared[T.int64(0), v0])"
        }
    }
]