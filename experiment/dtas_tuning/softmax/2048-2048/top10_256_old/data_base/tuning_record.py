#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0128    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0399    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0294    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0242    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0181    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0163    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0200    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0280    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0316    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1, 256)>)  latency(ms):    0.0209    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0452    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0233    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0262    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0274    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0186    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0371    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0385    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0398    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0292    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(257, 512)>)  latency(ms):    0.0329    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0457    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0414    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0552    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0299    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0352    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0333    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0287    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0446    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0281    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(513, 768)>)  latency(ms):    0.0457    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0364    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0365    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0585    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0527    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0486    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0408    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0403    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0402    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0422    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(769, 1024)>)  latency(ms):    0.0544    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0552    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0641    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0758    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0581    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0468    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0472    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0611    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0642    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0602    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1025, 1280)>)  latency(ms):    0.0521    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0544    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0623    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0741    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0558    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0689    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0678    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0800    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0733    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0582    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1281, 1536)>)  latency(ms):    0.0737    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0846    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0678    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0657    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0808    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0821    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0630    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0760    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0830    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0837    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1537, 1792)>)  latency(ms):    0.0652    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0707    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0878    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0878    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0918    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0783    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0731    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0862    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0895    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0811    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1, 256)>, <m: Range(1793, 2048)>)  latency(ms):    0.0753    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0790    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0856    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0906    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0332    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0569    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0695    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0432    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.1184    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0492    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1, 256)>)  latency(ms):    0.0562    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.1294    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.1034    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0796    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0912    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.1106    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0629    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0705    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.1074    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0763    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(257, 512)>)  latency(ms):    0.0509    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.1260    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.0795    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.1268    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.0740    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.1588    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.0944    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.0814    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.1277    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.1134    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(513, 768)>)  latency(ms):    0.0972    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1679    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.0963    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1092    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1116    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1488    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1531    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1389    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1213    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1076    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(769, 1024)>)  latency(ms):    0.1025    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1805    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1667    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1656    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1817    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1549    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1751    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1302    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1467    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.1333    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1025, 1280)>)  latency(ms):    0.2176    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1558    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1771    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.2059    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.2095    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1619    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1945    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.2300    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.2091    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1951    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1281, 1536)>)  latency(ms):    0.1538    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2339    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.1953    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2294    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2420    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.1877    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.1792    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.1848    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2356    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2161    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1537, 1792)>)  latency(ms):    0.2362    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2387    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2096    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2330    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2185    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2011    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2617    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2540    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2232    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2513    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(257, 512)>, <m: Range(1793, 2048)>)  latency(ms):    0.2576    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.1382    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.0911    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.1485    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.1136    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.1938    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.0793    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.1317    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.0534    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.0926    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1, 256)>)  latency(ms):    0.0696    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.2162    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1011    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1747    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.0830    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1133    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1687    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1493    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1247    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1810    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(257, 512)>)  latency(ms):    0.1299    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.2083    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.2063    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1600    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1338    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1290    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.2077    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1864    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1547    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.1223    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(513, 768)>)  latency(ms):    0.2623    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.2422    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1647    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.2276    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1815    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1989    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1570    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.2505    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1800    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.1818    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(769, 1024)>)  latency(ms):    0.2771    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2457    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2741    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2193    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.3573    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2233    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2990    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2898    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2423    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.2742    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1025, 1280)>)  latency(ms):    0.3050    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.2331    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3233    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3215    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3431    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.2908    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.2653    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3776    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3481    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.2725    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1281, 1536)>)  latency(ms):    0.3381    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.2724    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3209    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3885    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3095    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3949    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3573    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3891    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3793    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.4014    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1537, 1792)>)  latency(ms):    0.3153    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.3528    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.4248    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.3364    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.4299    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.3593    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.3458    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.4084    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.4166    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.4165    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(513, 768)>, <m: Range(1793, 2048)>)  latency(ms):    0.3859    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1919    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.2688    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1829    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1265    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.0733    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1093    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.0959    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1578    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.1282    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1, 256)>)  latency(ms):    0.2073    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.1786    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.3002    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.1569    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.1139    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.1720    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.2508    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.2070    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.2344    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.1411    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(257, 512)>)  latency(ms):    0.2427    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.1718    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.1838    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2838    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2587    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.1740    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.3640    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2908    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2139    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2209    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(513, 768)>)  latency(ms):    0.2897    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.3812    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2569    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.3478    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2551    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.3380    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2432    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.3163    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2373    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2208    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(769, 1024)>)  latency(ms):    0.2753    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.4070    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.5037    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.3599    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.4246    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.2952    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.3790    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.4025    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.3828    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.3075    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1025, 1280)>)  latency(ms):    0.3352    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4663    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.5324    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.3815    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.3760    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4704    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4037    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4473    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4488    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.3553    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1281, 1536)>)  latency(ms):    0.4816    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.5414    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.5719    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.5320    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.5457    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.4990    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.5436    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.4471    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.4397    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.4352    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1537, 1792)>)  latency(ms):    0.4223    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5604    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.4841    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5117    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.6015    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5842    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.4965    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5963    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5058    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5715    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(769, 1024)>, <m: Range(1793, 2048)>)  latency(ms):    0.5421    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.0914    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.1386    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.2489    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.1634    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.2341    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.2648    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.3457    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.2025    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.1621    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1, 256)>)  latency(ms):    0.1224    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.2003    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.2280    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.1459    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.3876    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.2660    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.1797    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.3099    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.3216    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.2219    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(257, 512)>)  latency(ms):    0.2994    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.3658    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.2235    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.3271    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.3702    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.4656    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.3673    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.2848    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.2390    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.2709    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(513, 768)>)  latency(ms):    0.2376    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.3986    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.3274    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.3127    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.3543    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.4925    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.4395    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.4483    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.3202    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.2846    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(769, 1024)>)  latency(ms):    0.2979    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.4775    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.6478    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.4066    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.5156    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.4626    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.3891    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.5390    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.5458    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.4343    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1025, 1280)>)  latency(ms):    0.4870    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.5551    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.4957    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.6184    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.5254    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.6074    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.5804    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.6215    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.4841    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.6766    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1281, 1536)>)  latency(ms):    0.4711    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.5111    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.7183    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.7057    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.6430    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.5470    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.5465    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.6980    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.7226    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.5756    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1537, 1792)>)  latency(ms):    0.6840    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.7313    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.7859    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.7428    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.6651    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.6487    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.7459    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.6247    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.6916    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.7668    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1025, 1280)>, <m: Range(1793, 2048)>)  latency(ms):    0.6167    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.1982    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.4194    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.2469    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.3228    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.1704    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.1994    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.1141    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.2870    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.3072    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1, 256)>)  latency(ms):    0.1493    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.3644    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.3784    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.2200    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.2778    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.1797    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.2436    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.2695    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.3905    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.4722    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(257, 512)>)  latency(ms):    0.3225    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.2467    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.2832    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.3437    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.4433    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.3355    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.4499    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.2905    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.4512    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.4010    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(513, 768)>)  latency(ms):    0.5672    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.5181    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.3525    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.4258    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.6081    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.3762    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.3872    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.5413    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.4970    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.4035    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(769, 1024)>)  latency(ms):    0.3807    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.4416    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.5596    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.7895    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.6554    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.4785    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.5974    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.5386    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.6675    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.5975    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1025, 1280)>)  latency(ms):    0.6383    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.5553    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.7445    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.7681    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.6269    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.8277    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.7700    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.5865    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.6984    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.7220    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1281, 1536)>)  latency(ms):    0.5588    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.5960    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.8783    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.8398    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.7165    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.8531    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.7798    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.8831    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.6959    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.8502    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1537, 1792)>)  latency(ms):    0.6918    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.9158    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.9561    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.8781    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.8325    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.7382    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.7742    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.9172    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.9294    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.7645    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1281, 1536)>, <m: Range(1793, 2048)>)  latency(ms):    0.8372    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.3398    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.4947    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.1996    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.1743    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.3793    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.2335    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.2910    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.2355    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.1341    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1, 256)>)  latency(ms):    0.3607    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.5560    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.4481    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.2595    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.2868    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.4601    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.3325    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.4297    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.3815    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.2088    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(257, 512)>)  latency(ms):    0.3172    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.3163    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.5365    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.5265    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.4111    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.6699    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.3986    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.5318    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.3415    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.4711    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(513, 768)>)  latency(ms):    0.3146    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.7030    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.4186    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.5058    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.5891    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.4715    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.4630    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.4410    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.4482    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.6428    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(769, 1024)>)  latency(ms):    0.6262    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.7325    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.7982    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.6708    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.5644    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.6991    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.7099    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.7769    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.6334    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.5809    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1025, 1280)>)  latency(ms):    0.9290    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.8527    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.9921    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.9030    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.6942    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.6712    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.6895    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.7507    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.8931    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.8410    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1281, 1536)>)  latency(ms):    0.8417    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.9763    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.9970    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    1.0206    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.8210    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    1.0088    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.7845    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.8238    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.9233    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    0.8210    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1537, 1792)>)  latency(ms):    1.0378    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    1.0203    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    1.1322    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    1.1091    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    1.0930    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    0.9960    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    0.8802    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    0.9628    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    1.0805    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    0.9424    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1537, 1792)>, <m: Range(1793, 2048)>)  latency(ms):    0.9201    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.5918    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.2298    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.4367    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.3884    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.1544    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.4147    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.2688    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.3355    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.2018    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1, 256)>)  latency(ms):    0.2707    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.3224    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.3674    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.6409    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.2445    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.4968    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.5160    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.3807    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.2969    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.5301    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(257, 512)>)  latency(ms):    0.4404    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.5328    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.6049    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.6182    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.4560    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.4649    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.3968    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.3648    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.6150    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.3913    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(513, 768)>)  latency(ms):    0.7751    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.5177    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.5318    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.8204    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.6837    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.5278    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.4667    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.5033    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.7177    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.5899    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(769, 1024)>)  latency(ms):    0.7400    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'cache'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.6177    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.9207    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.8659    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    1.0723    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.7156    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.8914    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.8246    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.8113    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.6640    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1025, 1280)>)  latency(ms):    0.7582    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.9944    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.9669    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    1.0141    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.8367    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.9543    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    1.0355    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.7947    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.8690    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    1.1295    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1281, 1536)>)  latency(ms):    0.8262    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.1191    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.1951    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    0.9606    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.1473    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    0.9516    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.1354    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    0.9619    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.1934    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    1.0798    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1537, 1792)>)  latency(ms):    0.9070    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.2219    
#config: ReductionConfig: {'len_tx': 352, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) + T.int64(1407) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1))) // T.int64(1408)):
                for ax3_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1408) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)) + ((ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(352) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(351)) // T.int64(352) * T.int64(352)) - T.min(T.int64(0), (m + T.int64(351)) // T.int64(352) * T.int64(352) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(352) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(352) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(352), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(351)) // T.int64(352), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(352) + ax2_1)
                        T.where(ax2_0 * T.int64(352) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.1184    
#config: ReductionConfig: {'len_tx': 192, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) + T.int64(767) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1))) // T.int64(768)):
                for ax3_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(768) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)) + ((ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(192) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(191)) // T.int64(192) * T.int64(192)) - T.min(T.int64(0), (m + T.int64(191)) // T.int64(192) * T.int64(192) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(192), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)
                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.2840    
#config: ReductionConfig: {'len_tx': 384, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) + T.int64(1535) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1))) // T.int64(1536)):
                for ax3_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1536) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)) + ((ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(384) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(383)) // T.int64(384) * T.int64(384)) - T.min(T.int64(0), (m + T.int64(383)) // T.int64(384) * T.int64(384) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(384) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(384) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(384), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(383)) // T.int64(384), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(384) + ax2_1)
                        T.where(ax2_0 * T.int64(384) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.0269    
#config: ReductionConfig: {'len_tx': 128, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)
                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.2482    
#config: ReductionConfig: {'len_tx': 512, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) + T.int64(2047) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1))) // T.int64(2048)):
                for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(2048) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)) + ((ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(512) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(511)) // T.int64(512) * T.int64(512)) - T.min(T.int64(0), (m + T.int64(511)) // T.int64(512) * T.int64(512) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(512) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(512) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(511)) // T.int64(512), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(512) + ax2_1)
                        T.where(ax2_0 * T.int64(512) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.2578    
#config: ReductionConfig: {'len_tx': 320, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) + T.int64(1279) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1))) // T.int64(1280)):
                for ax3_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1280) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)) + ((ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(320) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(319)) // T.int64(320) * T.int64(320)) - T.min(T.int64(0), (m + T.int64(319)) // T.int64(320) * T.int64(320) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(320) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(320) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(320), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(319)) // T.int64(320), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(320) + ax2_1)
                        T.where(ax2_0 * T.int64(320) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.1395    
#config: ReductionConfig: {'len_tx': 288, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) + T.int64(1151) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1))) // T.int64(1152)):
                for ax3_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1152) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)) + ((ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(288) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(287)) // T.int64(288) * T.int64(288)) - T.min(T.int64(0), (m + T.int64(287)) // T.int64(288) * T.int64(288) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(288) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(288) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(288), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(287)) // T.int64(288), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(288) + ax2_1)
                        T.where(ax2_0 * T.int64(288) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.1211    
#config: ReductionConfig: {'len_tx': 224, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) + T.int64(895) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1))) // T.int64(896)):
                for ax3_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(896) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)) + ((ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(224) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(223)) // T.int64(224) * T.int64(224)) - T.min(T.int64(0), (m + T.int64(223)) // T.int64(224) * T.int64(224) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(224) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(224) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(223)) // T.int64(224), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(224) + ax2_1)
                        T.where(ax2_0 * T.int64(224) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.0568    
#config: ReductionConfig: {'len_tx': 160, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):
                for ax3_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)
                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
#name: softmax
#range: (<n: Range(1793, 2048)>, <m: Range(1793, 2048)>)  latency(ms):    1.2154    
#config: ReductionConfig: {'len_tx': 256, 'unroll_depth': 256, 'vector_size': 4, 'temp_storage': 'shared.dyn'}
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def main(p_lv38: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        n, m = T.int64(), T.int64()
        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))
        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), "float16")
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope="shared")
        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope="shared.dyn")
        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) + T.int64(1023) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1))) // T.int64(1024)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax3_2 in T.vectorized(T.int64(4)):
                        with T.block("lv38_shared.dyn"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)
                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)
                            v3 = T.axis.spatial(m, ax3_0 * T.int64(1024) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)), T.int64(0))))
                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)) + ((ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(256) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(255)) // T.int64(256) * T.int64(256)) - T.min(T.int64(0), (m + T.int64(255)) // T.int64(256) * T.int64(256) - T.int64(1)))
                            T.reads(lv38[v0, v1, v2, v3])
                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])
                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_maxelem"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])
                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                        with T.block("T_softmax_expsum"):
                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)
                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)
                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(256) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(256) + ax2_fused_1 < m)
                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])
                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])
                            with T.init():
                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)
                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])
            for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax2_0 in T.serial((m + T.int64(255)) // T.int64(256), annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                    with T.block("compute"):
                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)
                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)
                        v2 = T.axis.spatial(m, ax2_0 * T.int64(256) + ax2_1)
                        T.where(ax2_0 * T.int64(256) + ax2_1 < m)
                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])
                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])
                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast("float16", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])
 
