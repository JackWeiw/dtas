[
    {
        "softmax_<n: Range(1, 256)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.01294865602673147,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.01813848165640631,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.027726944466298108,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.039954810759825335,
            "config": [
                {
                    "len_tx": 192,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(192) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(192) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(192), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(191)) // T.int64(192), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(192) + ax2_1)\n                        T.where(ax2_0 * T.int64(192) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.04596608031576789,
            "config": [
                {
                    "len_tx": 160,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) + T.int64(639) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1))) // T.int64(640)):\n                for ax3_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(640) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)) + ((ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(160) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(159)) // T.int64(160) * T.int64(160)) - T.min(T.int64(0), (m + T.int64(159)) // T.int64(160) * T.int64(160) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(160) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(160) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(160), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(159)) // T.int64(160), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(160) + ax2_1)\n                        T.where(ax2_0 * T.int64(160) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.052101181922980536,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.06081062701498583,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1, 256)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.06652625163059164,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.03387986713223699,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.05000162255813954,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.07215767019381875,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.0977596172008547,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.1278369174382716,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.14390024827586206,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.16727996875816992,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(257, 512)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.1843481848623853,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.054433073254759747,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.08097393035522067,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.11605893895348836,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.15411025936170214,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.20827340742857142,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.23547040348432055,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.2712770861940299,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(513, 768)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.2960653232758621,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.0747033808551372,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.11155998959731543,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.15989252929292927,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.2100344297994269,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.2874339053672316,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.32354274739336497,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.3783401728260869,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(769, 1024)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.41162272088607593,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.09420051147959183,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.1414591468487395,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.20308730263157898,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.28682884333333336,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.3684078171122994,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.41261968253275116,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.4836830827338129,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1025, 1280)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.5276233117647059,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.11473230817307693,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.1720746875989446,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.2466281877076412,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.34606455321100915,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.44835480764331204,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.5024828607526881,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.58806053539823,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1281, 1536)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.6329298923076924,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.13494138736462094,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.20271290767634853,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.2929767147286822,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.40347802499999996,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.5300520214876034,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.5932443375,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.6820876107142857,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1537, 1792)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.7524570172131146,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(1, 256)>": {
            "latency(ms)": 0.15548744485596705,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(257, 512)>": {
            "latency(ms)": 0.23617995241379308,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(513, 768)>": {
            "latency(ms)": 0.32626966545454544,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(769, 1024)>": {
            "latency(ms)": 0.47362021082802547,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "cache"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(1025, 1280)>": {
            "latency(ms)": 0.6122560044642857,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(1281, 1536)>": {
            "latency(ms)": 0.6790178077777776,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(1537, 1792)>": {
            "latency(ms)": 0.789603631081081,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    },
    {
        "softmax_<n: Range(1793, 2048)>_<m: Range(1793, 2048)>": {
            "latency(ms)": 0.863836008695652,
            "config": [
                {
                    "len_tx": 128,
                    "unroll_depth": 256,
                    "vector_size": 4,
                    "temp_storage": "shared.dyn"
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv38: T.handle, p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n, m = T.int64(), T.int64()\n        lv38 = T.match_buffer(p_lv38, (T.int64(1), T.int64(32), n, m))\n        var_compute_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(32), n, m), \"float16\")\n        # with T.block(\"root\"):\n        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(32), n), scope=\"shared\")\n        lv38_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(32), n, m), scope=\"shared.dyn\")\n        for ax0_ax1_fused in T.thread_binding(n * T.int64(32), thread=\"blockIdx.x\"):\n            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), (T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) + T.int64(511) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1))) // T.int64(512)):\n                for ax3_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax3_2 in T.vectorized(T.int64(4)):\n                        with T.block(\"lv38_shared.dyn\"):\n                            v0 = T.axis.spatial(T.int64(1), ax0)\n                            v1 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax1)\n                            v2 = T.axis.spatial(n, ax0_ax1_fused % n + ax2)\n                            v3 = T.axis.spatial(m, ax3_0 * T.int64(512) + ax3_1 * T.int64(4) + ax3_2 + (T.int64(0) - (T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))) + (T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + T.max(T.int64(0) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)), T.int64(0))))\n                            T.where(T.int64(0) <= T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) and T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)) + ((ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2) < m and (ax3_0 * T.int64(128) + ax3_1) * T.int64(4) + ax3_2 < T.max(T.int64(1), (m + T.int64(127)) // T.int64(128) * T.int64(128)) - T.min(T.int64(0), (m + T.int64(127)) // T.int64(128) * T.int64(128) - T.int64(1)))\n                            T.reads(lv38[v0, v1, v2, v3])\n                            T.writes(lv38_shared_dyn[v0, v1, v2, v3])\n                            lv38_shared_dyn[v0, v1, v2, v3] = lv38[v0, v1, v2, v3]\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_maxelem\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2])\n                            T.writes(T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.float32(-3.4028234663852886e+38)\n                            T_softmax_maxelem_shared[T.int64(0), v0, v1] = T.max(T_softmax_maxelem_shared[T.int64(0), v0, v1], lv38_shared_dyn[T.int64(0), v0, v1, v2])\n            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):\n                for ax2_fused_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                    for ax2_fused_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                        with T.block(\"T_softmax_expsum\"):\n                            v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n + ax0)\n                            v1 = T.axis.spatial(n, ax0_ax1_fused % n + ax1)\n                            v2 = T.axis.reduce(m, ax2_fused_0 * T.int64(128) + ax2_fused_1)\n                            T.where(ax2_fused_0 * T.int64(128) + ax2_fused_1 < m)\n                            T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1])\n                            T.writes(T_softmax_expsum_shared[T.int64(0), v0, v1])\n                            with T.init():\n                                T_softmax_expsum_shared[T.int64(0), v0, v1] = T.float32(0)\n                            T_softmax_expsum_shared[T.int64(0), v0, v1] = T_softmax_expsum_shared[T.int64(0), v0, v1] + T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1])\n            for ax2_1 in T.thread_binding(T.int64(128), thread=\"threadIdx.x\"):\n                for ax2_0 in T.serial((m + T.int64(127)) // T.int64(128), annotations={\"pragma_auto_unroll_max_step\": 256, \"pragma_unroll_explicit\": 1}):\n                    with T.block(\"compute\"):\n                        v0 = T.axis.spatial(T.int64(32), ax0_ax1_fused // n)\n                        v1 = T.axis.spatial(n, ax0_ax1_fused % n)\n                        v2 = T.axis.spatial(m, ax2_0 * T.int64(128) + ax2_1)\n                        T.where(ax2_0 * T.int64(128) + ax2_1 < m)\n                        T.reads(lv38_shared_dyn[T.int64(0), v0, v1, v2], T_softmax_maxelem_shared[T.int64(0), v0, v1], T_softmax_expsum_shared[T.int64(0), v0, v1])\n                        T.writes(var_compute_intermediate[T.int64(0), v0, v1, v2])\n                        var_compute_intermediate[T.int64(0), v0, v1, v2] = T.Cast(\"float16\", T.exp(lv38_shared_dyn[T.int64(0), v0, v1, v2] - T_softmax_maxelem_shared[T.int64(0), v0, v1]) / T_softmax_expsum_shared[T.int64(0), v0, v1])"
        }
    }
]