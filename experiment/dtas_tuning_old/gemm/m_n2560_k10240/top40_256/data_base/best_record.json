[
    {
        "gemm_<n: Range(1, 256)>": {
            "latency(ms)": 0.5812826977777777,
            "config": [
                {
                    "tile_shape": [
                        160,
                        128
                    ],
                    "performance": -127771.34598012647,
                    "assin_score": 64,
                    "fma_ldg_ratio": 71.11111111111111,
                    "l2_hit_rate": 0.8577235772357723,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 5,
                    "j": 4,
                    "micro_shape": [
                        2,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(159)) // T.int64(160), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(4)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(257, 512)>": {
            "latency(ms)": 0.6256675859649123,
            "config": [
                {
                    "tile_shape": [
                        160,
                        128
                    ],
                    "performance": -127771.34598012647,
                    "assin_score": 128,
                    "fma_ldg_ratio": 71.11111111111111,
                    "l2_hit_rate": 0.8577235772357723,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 5,
                    "j": 4,
                    "micro_shape": [
                        2,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(159)) // T.int64(160), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(4)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(513, 768)>": {
            "latency(ms)": 0.6974767883870967,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1, 256)>": {
            "latency(ms)": 0.5837643681818181,
            "config": [
                {
                    "tile_shape": [
                        160,
                        128
                    ],
                    "performance": -127771.34598012647,
                    "assin_score": 64,
                    "fma_ldg_ratio": 71.11111111111111,
                    "l2_hit_rate": 0.8577235772357723,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 10,
                    "j": 2,
                    "micro_shape": [
                        1,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(159)) // T.int64(160), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(1), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(10) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(10) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(1)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(10) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(10) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(1), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(10) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(10) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(1), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(10) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(10) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(4)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + ax2_0_2_ax1_0_2_fused % T.int64(10) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1, 256)>": {
            "latency(ms)": 0.2545257069536424,
            "config": [
                {
                    "tile_shape": [
                        160,
                        128
                    ],
                    "performance": -127771.34598012647,
                    "assin_score": 64,
                    "fma_ldg_ratio": 71.11111111111111,
                    "l2_hit_rate": 0.8577235772357723,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 5,
                    "j": 4,
                    "micro_shape": [
                        2,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(159)) // T.int64(160), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(5120) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(20), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(20) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(4)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(257, 512)>": {
            "latency(ms)": 0.29320615662650606,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 64,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(513, 768)>": {
            "latency(ms)": 0.3306576717842324,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 3,
                    "j": 4,
                    "micro_shape": [
                        4,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(769, 1024)>": {
            "latency(ms)": 0.5930016251968505,
            "config": [
                {
                    "tile_shape": [
                        160,
                        128
                    ],
                    "performance": -127771.34598012647,
                    "assin_score": 96,
                    "fma_ldg_ratio": 71.11111111111111,
                    "l2_hit_rate": 0.8577235772357723,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 5,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(159)) // T.int64(160) * T.int64(160), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(159)) // T.int64(160), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(10), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(10), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + (ax1_ax2_fused_0_0_0 * T.int64(2560) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(2560) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(10), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(10) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(10) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(10) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(10), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(10) * ((n + T.int64(159)) // T.int64(160)), ax1_0_0_ax2_0_0_fused * T.int64(10) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(159)) // T.int64(160) * T.int64(160), ax1_0_0_ax2_0_0_fused * T.int64(160) + ax2_0_2_ax1_0_2_fused % T.int64(5) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1025, 1280)>": {
            "latency(ms)": 0.6219215182539682,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 64,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1281, 1536)>": {
            "latency(ms)": 0.6656362085365852,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 3,
                    "j": 4,
                    "micro_shape": [
                        4,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1537, 1792)>": {
            "latency(ms)": 0.8377061918367346,
            "config": [
                {
                    "tile_shape": [
                        224,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 81.45454545454545,
                    "l2_hit_rate": 0.8725055432372506,
                    "wmma_shape": [
                        8,
                        32,
                        16
                    ],
                    "i": 7,
                    "j": 1,
                    "micro_shape": [
                        4,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(895)) // T.int64(896) * T.int64(4), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(112), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                    T.tvm_fill_fragment(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(8)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(896), ax1_0_0_ax2_0_0_fused * T.int64(224) + (ax1_ax2_fused_0_0_0 * T.int64(1792) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1792) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(5)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(28) * ((n + T.int64(895)) // T.int64(896) * T.int64(4)), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(8), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(32) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(32), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(112), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32), A.data, A.elem_offset // A.strides[0] // T.int64(8) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(32) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(28) * ((n + T.int64(895)) // T.int64(896) * T.int64(4)), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=32)\n                                T.tvm_store_matrix_sync(A.data, 8, 32, 16, A.elem_offset // A.strides[0] // T.int64(8) * (A.strides[0] // T.int64(32)) + A.elem_offset % A.strides[0] // T.int64(32), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(8), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(16)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(896), ax1_0_0_ax2_0_0_fused * T.int64(224) + ax2_0_2_ax1_0_2_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(128))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(128))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 32, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(1793, 2048)>": {
            "latency(ms)": 0.9710762607142855,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 64,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 3,
                    "j": 4,
                    "micro_shape": [
                        4,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(2049, 2304)>": {
            "latency(ms)": 0.9954491158536584,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(2305, 2560)>": {
            "latency(ms)": 1.2535657902439024,
            "config": [
                {
                    "tile_shape": [
                        224,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 128,
                    "fma_ldg_ratio": 81.45454545454545,
                    "l2_hit_rate": 0.8725055432372506,
                    "wmma_shape": [
                        8,
                        32,
                        16
                    ],
                    "i": 7,
                    "j": 1,
                    "micro_shape": [
                        4,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(895)) // T.int64(896) * T.int64(896), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(895)) // T.int64(896) * T.int64(4), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(112), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                    T.tvm_fill_fragment(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(8)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(896), ax1_0_0_ax2_0_0_fused * T.int64(224) + (ax1_ax2_fused_0_0_0 * T.int64(1792) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(1792) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(5)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(7), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(7) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(28) * ((n + T.int64(895)) // T.int64(896) * T.int64(4)), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(8), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(32):v1_o * T.int64(32) + T.int64(32), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 8, 32, 16, C.elem_offset // C.strides[0] // T.int64(32) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(32), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(112), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(8), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(32), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32), A.data, A.elem_offset // A.strides[0] // T.int64(8) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(32) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(8) * (C.strides[0] // T.int64(32)) + C.elem_offset % C.strides[0] // T.int64(32))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(28) * ((n + T.int64(895)) // T.int64(896) * T.int64(4)), ax1_0_0_ax2_0_0_fused * T.int64(28) + ax2_0_2_ax1_0_2_fused * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(80), ax1_0_1_ax2_0_1_fused * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=32)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(8):v1_o * T.int64(8) + T.int64(8), v2_o * T.int64(32):v2_o * T.int64(32) + T.int64(32)], (T.int64(8), T.int64(32)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=32)\n                                T.tvm_store_matrix_sync(A.data, 8, 32, 16, A.elem_offset // A.strides[0] // T.int64(8) * (A.strides[0] // T.int64(32)) + A.elem_offset % A.strides[0] // T.int64(32), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(8), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(16)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(895)) // T.int64(896) * T.int64(896), ax1_0_0_ax2_0_0_fused * T.int64(224) + ax2_0_2_ax1_0_2_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(128))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(128))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 32, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(2561, 2816)>": {
            "latency(ms)": 1.3271072682539682,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 64,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(2817, 3072)>": {
            "latency(ms)": 1.3407881195121953,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 4,
                    "j": 4,
                    "micro_shape": [
                        3,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(3), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(3)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(3), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(3), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(6)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(48) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(3073, 3328)>": {
            "latency(ms)": 1.6409579360000002,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 128,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 3,
                    "j": 4,
                    "micro_shape": [
                        4,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(4), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(4), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(4) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(3) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(3) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(3329, 3584)>": {
            "latency(ms)": 1.6652519064516127,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 64,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(3585, 3840)>": {
            "latency(ms)": 1.69311678125,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 0,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 6,
                    "j": 2,
                    "micro_shape": [
                        2,
                        4,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(4)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(3072) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(12), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + ((ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1) * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.where(ax1_ax2_fused_0_0_0 * T.int64(12) + ax1_ax2_fused_0_0_1 < T.int64(32))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(4)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(2), T.int64(4)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(2) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(4) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(8)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(6) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(64))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(6) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(64))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    },
    {
        "gemm_<n: Range(3841, 4096)>": {
            "latency(ms)": 2.001933497368421,
            "config": [
                {
                    "tile_shape": [
                        192,
                        128
                    ],
                    "performance": -142000000000000.0,
                    "assin_score": 128,
                    "fma_ldg_ratio": 76.8,
                    "l2_hit_rate": 0.8658536585365854,
                    "wmma_shape": [
                        16,
                        16,
                        16
                    ],
                    "i": 4,
                    "j": 4,
                    "micro_shape": [
                        3,
                        2,
                        4
                    ],
                    "in_vec_len_a": 8,
                    "in_vec_len_b": 8,
                    "in_pad": 8,
                    "out_vec_len_pad": [
                        8,
                        4
                    ],
                    "warp_size": 32,
                    "use_software_pipeline": true,
                    "use_double_buffer": true,
                    "manifest_shared_memory_local_stage": true,
                    "use_async_copy": false
                }
            ],
            "mod": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func(private=True)\n    def main(p_lv9: T.handle, lv3: T.Buffer((T.int64(2560), T.int64(10240)), \"float16\"), lv4: T.Buffer((T.int64(2560),), \"float16\"), p_output0: T.handle):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        n = T.int64()\n        lv9 = T.match_buffer(p_lv9, (T.int64(1), n, T.int64(10240)), \"float16\")\n        var_T_add_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(2560)), \"float16\")\n        # with T.block(\"root\"):\n        lv9_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv3_reindex_shared_dyn = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"shared.dyn\")\n        lv9_reindex_pad_shared_dyn_wmma_matrix_a = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(10240)), \"float16\", scope=\"wmma.matrix_a\")\n        lv3_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((T.int64(1), T.int64(2560), T.int64(10240)), \"float16\", scope=\"wmma.matrix_b\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"shared.dyn\")\n        var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator = T.alloc_buffer((T.int64(1), (n + T.int64(191)) // T.int64(192) * T.int64(192), T.int64(2560)), \"float16\", scope=\"wmma.accumulator\")\n        for ax0 in T.thread_binding(T.int64(1), thread=\"blockIdx.z\"):\n            for ax1_0_0_ax2_0_0_fused in T.thread_binding((n + T.int64(191)) // T.int64(192), thread=\"blockIdx.x\"):\n                for ax1_0_1_ax2_0_1_fused in T.thread_binding(T.int64(20), thread=\"blockIdx.y\"):\n                    for ax2_0_2_ax1_0_2_fused in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                        for ax1_0_3_init, ax2_0_3_init in T.grid(T.int64(3), T.int64(2)):\n                            with T.block(\"NT_matmul_o_init\"):\n                                v0_o = T.axis.spatial(T.int64(1), ax0)\n                                v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax1_0_3_init)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3_init)\n                                T.reads()\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                with T.block(\"NT_matmul_init_o\"):\n                                    v1_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    v2_i_init_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                    T.reads()\n                                    T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                    C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                    T.tvm_fill_fragment(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.float32(0))\n                        for ax3_0_0 in T.serial(T.int64(160), annotations={\"software_pipeline_order\": [0, 3, 1, 4, 5, 2, 6], \"software_pipeline_stage\": [0, 0, 0, 0, 0, 1, 1]}):\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(3)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv9_reindex_pad_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv9[T.int64(0), v1, v2])\n                                                T.writes(lv9_reindex_pad_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv9_reindex_pad_shared_dyn[v0, v1, v2] = T.if_then_else(v1 < n, lv9[T.int64(0), v1, v2], T.float16(0))\n                            for ax0_1, ax1_ax2_fused_0_0_0 in T.grid(T.int64(1), T.int64(2)):\n                                for ax1_ax2_fused_0_0_1 in T.thread_binding(T.int64(16), thread=\"threadIdx.y\"):\n                                    for ax1_ax2_fused_0_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                        for ax1_ax2_fused_1 in T.vectorized(T.int64(8), annotations={\"check_vector_load\": 1, \"remove_vector_condition\": 1}):\n                                            with T.block(\"lv3_reindex_shared.dyn\"):\n                                                v0 = T.axis.spatial(T.int64(1), ax0_1)\n                                                v1 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) // T.int64(64))\n                                                v2 = T.axis.spatial(T.int64(10240), ax3_0_0 * T.int64(64) + (ax1_ax2_fused_0_0_0 * T.int64(4096) + ax1_ax2_fused_0_0_1 * T.int64(256) + ax1_ax2_fused_0_1 * T.int64(8) + ax1_ax2_fused_1) % T.int64(64))\n                                                T.reads(lv3[v1, v2])\n                                                T.writes(lv3_reindex_shared_dyn[v0, v1, v2])\n                                                T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 8]], \"double_buffer_scope\": 0, \"tir.manifest_shared_memory_local_stage\": 1})\n                                                lv3_reindex_shared_dyn[v0, v1, v2] = lv3[v1, v2]\n                            for ax3_0_1 in T.serial(T.int64(4), annotations={\"software_pipeline_order\": [0, 1, 2], \"software_pipeline_stage\": [0, 0, 1]}):\n                                for ax0_0 in T.unroll(T.int64(3)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv9_reindex_pad_shared.dyn_wmma.matrix_a_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"row_major\")\n                                for ax0_0 in T.unroll(T.int64(2)):\n                                    for ax1_0 in T.unroll(T.int64(1)):\n                                        with T.block(\"lv3_reindex_shared.dyn_wmma.matrix_b_o\"):\n                                            v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v1_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax0_0)\n                                            v2_o = T.axis.spatial(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1 + ax1_0)\n                                            T.reads(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            T.writes(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv3_reindex_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                            C = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            T.tvm_load_matrix_sync(C.data, 16, 16, 16, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), A.data, A.elem_offset, A.strides[0] * T.int64(16), 1), A.strides[0], \"col_major\")\n                                for ax1_0_3, ax2_0_3 in T.grid(T.int64(3), T.int64(2)):\n                                    with T.block(\"NT_matmul_o_update\"):\n                                        v0_o = T.axis.spatial(T.int64(1), ax0)\n                                        v1_o = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(12), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax1_0_3)\n                                        v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax2_0_3)\n                                        v3_o = T.axis.reduce(T.int64(640), ax3_0_0 * T.int64(4) + ax3_0_1)\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                        T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                        with T.block(\"NT_matmul_o\"):\n                                            v1_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v2_i_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                            v3_i_o = T.axis.reduce(T.int64(1), T.int64(0))\n                                            T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)])\n                                            T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                            A = T.match_buffer(lv9_reindex_pad_shared_dyn_wmma_matrix_a[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.matrix_a\", offset_factor=16)\n                                            B = T.match_buffer(lv3_reindex_shared_dyn_wmma_matrix_b[T.int64(0), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16):v3_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"B_s0\", \"B_s1\"), scope=\"wmma.matrix_b\", offset_factor=16)\n                                            C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[T.int64(0), v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                            T.tvm_mma_sync(C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16), A.data, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), B.data, B.elem_offset // B.strides[0] // T.int64(16) * (B.strides[0] // T.int64(16)) + B.elem_offset % B.strides[0] // T.int64(16), C.data, C.elem_offset // C.strides[0] // T.int64(16) * (C.strides[0] // T.int64(16)) + C.elem_offset % C.strides[0] // T.int64(16))\n                        for ax0_0, ax1_0 in T.grid(T.int64(3), T.int64(2)):\n                            with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn_wmma.accumulator_o\"):\n                                v0_o = T.axis.spatial(T.int64(1), T.int64(0))\n                                v1_o = T.axis.spatial(T.int64(12) * ((n + T.int64(191)) // T.int64(192)), ax1_0_0_ax2_0_0_fused * T.int64(12) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(3) + ax0_0)\n                                v2_o = T.axis.spatial(T.int64(160), ax1_0_1_ax2_0_1_fused * T.int64(8) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(2) + ax1_0)\n                                T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                T.writes(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)])\n                                A = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn_wmma_accumulator[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"A_s0\", \"A_s1\"), scope=\"wmma.accumulator\", offset_factor=16)\n                                C = T.match_buffer(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0_o, v1_o * T.int64(16):v1_o * T.int64(16) + T.int64(16), v2_o * T.int64(16):v2_o * T.int64(16) + T.int64(16)], (T.int64(16), T.int64(16)), \"float16\", strides=(\"C_s0\", \"C_s1\"), scope=\"shared.dyn\", offset_factor=16)\n                                T.tvm_store_matrix_sync(A.data, 16, 16, 16, A.elem_offset // A.strides[0] // T.int64(16) * (A.strides[0] // T.int64(16)) + A.elem_offset % A.strides[0] // T.int64(16), T.tvm_access_ptr(T.type_annotation(\"float16\"), C.data, C.elem_offset, C.strides[0] * T.int64(16), 2), C.strides[0], \"row_major\")\n                        for ax0_ax1_fused_0 in range(T.int64(6)):\n                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n                                for ax0_ax1_fused_2 in T.vectorized(T.int64(8)):\n                                    with T.block(\"var_NT_matmul_intermediate_reindex_pad_shared.dyn\"):\n                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))\n                                        v1 = T.axis.spatial((n + T.int64(191)) // T.int64(192) * T.int64(192), ax1_0_0_ax2_0_0_fused * T.int64(192) + ax2_0_2_ax1_0_2_fused % T.int64(4) * T.int64(48) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) // T.int64(32))\n                                        v2 = T.axis.spatial(T.int64(2560), ax1_0_1_ax2_0_1_fused * T.int64(128) + ax2_0_2_ax1_0_2_fused // T.int64(4) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(8) + ax0_ax1_fused_2) % T.int64(32))\n                                        T.reads(var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2], lv4[v2])\n                                        T.writes(var_T_add_intermediate[T.int64(0), v1, v2])\n                                        T.block_attr({\"buffer_dim_align\": [[0, 1, 16, 4]]})\n                                        if v1 < n:\n                                            var_T_add_intermediate[T.int64(0), v1, v2] = var_NT_matmul_intermediate_reindex_pad_shared_dyn[v0, v1, v2] + lv4[v2]"
        }
    }
]